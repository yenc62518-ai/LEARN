{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "44fe7949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting import_lib.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile import_lib.py\n",
    "'''\n",
    "File tải thư viện cho Lightning và các Module cho bài phân loại multiclass\n",
    "'''\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torchvision.tv_tensors import Image as TVImage\n",
    "from torchvision.transforms import v2 as TV2\n",
    "from torchmetrics import F1Score,ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "import pytorch_lightning as PL\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.tuner import Tuner\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image as PILImage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274ada0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataloader/datamodule.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataloader/datamodule.py\n",
    "'''\n",
    "Class tải dữ liệu:\n",
    "\n",
    "Tham số: \n",
    "batch_size (int)\n",
    "data_dir: đường dẫn đến data (Path)\n",
    "\n",
    "Trả về:\n",
    "prepare_data(): tải dữ liệu được ghi trong data_dir\n",
    "setup(): chuẩn bị dữ liệu dựa trên hàm gọi \n",
    "data_loader(): trả về data_loader với data là train/test/val \n",
    "'''\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Subset\n",
    "class DataModule(PL.LightningDataModule) :\n",
    "    def __init__(self, batch_size=32, data_dir = Path(\"pizza_steak_sushi/\")):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data_dir = data_dir\n",
    "        self.train_transform = TV2.Compose([\n",
    "            TV2.Resize((64,64)),\n",
    "            TV2.ToTensor()\n",
    "            ])\n",
    "        self.test_transform = TV2.Compose([\n",
    "            TV2.Resize((64,64)),\n",
    "            TV2.ToTensor()\n",
    "            ])\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "    def setup(self, stage = None):\n",
    "        if (stage == 'fit' or stage is None):\n",
    "            full_train_ds = ImageFolder(root=self.data_dir / 'train', transform=self.train_transform)\n",
    "            train_len = int(0.8 * len(full_train_ds))\n",
    "            val_len = len(full_train_ds) - train_len\n",
    "            self.train_ds, self.val_ds = random_split(full_train_ds, [train_len, val_len])\n",
    "            # train_subset, val_subset = random_split(full_train_ds, [train_len, val_len])\n",
    "            # self.train_ds = Subset(\n",
    "            #                     ImageFolder(root=self.data_dir / 'train', transform=self.train_transform),\n",
    "            #                     indices = train_subset.indices)\n",
    "            # self.val_ds = Subset(\n",
    "            #                     ImageFolder(root=self.data_dir / 'train', transform=self.test_transform),\n",
    "            #                     indices=val_subset.indices)\n",
    "\n",
    "        if (stage == 'test' or stage is None):\n",
    "            self.test_ds = ImageFolder(root = self.data_dir / 'test', transform=self.test_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset = self.train_ds, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(dataset = self.test_ds, batch_size=self.batch_size, num_workers=2)\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset = self.val_ds, batch_size=self.batch_size, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fefb958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing backbones/model_class.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backbones/model_class.py\n",
    "'''\n",
    "Class khai báo model TinyVGG:\n",
    "\n",
    "Tham số:\n",
    "input: channel ảnh (int)\n",
    "output_size: số lượng label (int)\n",
    "kernel: số filter (int)\n",
    "lr (float)\n",
    "\n",
    "Cấu trúc: \n",
    "1 Conv2d (input, kernel, 3-same)\n",
    "3 Conv2d (kernel, kernel, 3-same)\n",
    "2 MaxPool2d (2)\n",
    "Flatten()\n",
    "Linear(kernel*H*W/4, output_size)\n",
    "\n",
    "Trả về:\n",
    "forward của cấu trúc\n",
    "training_step() với CEL, f1, adam(lr)\n",
    "test_step(), validation_step() với CEL, f1\n",
    "'''\n",
    "\n",
    "class TinyVGG(PL.LightningModule) :\n",
    "    def __init__(self, input, output_size, kernel, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.train_f1 = F1Score(task = 'multiclass', num_classes=output_size)\n",
    "        self.val_f1 = F1Score(task = 'multiclass', num_classes=output_size)\n",
    "        self.test_f1 = F1Score(task = 'multiclass', num_classes=output_size)\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input,\n",
    "                      out_channels=kernel,\n",
    "                      kernel_size=3,\n",
    "                      padding = 'same',\n",
    "                      stride = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=kernel,\n",
    "                      out_channels=kernel,\n",
    "                      kernel_size=3,\n",
    "                      padding = 'same',\n",
    "                      stride = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2) # 32, 16, 14, 14\n",
    "        )\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(kernel,\n",
    "                      kernel,\n",
    "                      kernel_size=3,\n",
    "                      padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d((kernel),\n",
    "                      (kernel),\n",
    "                      kernel_size=3,\n",
    "                      padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2) # 32, 16, 7, 7\n",
    "        )\n",
    "        self.classification = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((kernel)*16*16, output_size)\n",
    "        )\n",
    "    def forward(self, X) :\n",
    "        X = self.block_1(X)\n",
    "        X = self.block_2(X)\n",
    "        X = self.classification(X)\n",
    "        return X\n",
    "    \n",
    "    def training_step(self, batch, batch_idx) :\n",
    "        X,y = batch\n",
    "        logits = self(X) # đang tự gọi hàm forward của nó\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        self.train_f1.update(pred, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', self.train_f1, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        logits = self(X)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_f1.update(preds, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_f1\", self.val_f1, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        logits = self(X) # đang tự gọi hàm forward của nó\n",
    "        pred = torch.argmax(logits, dim = 1)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        self.test_f1.update(pred, y)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', self.test_f1, prog_bar=True)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        X, label = batch\n",
    "        return self(X)\n",
    "    \n",
    "    def configure_optimizers(self): \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr) \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.9) \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ec348b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logger.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile logger.py \n",
    "'''\n",
    "Hàm khai báo các phương thức Logger, CheckPoint, EarlyStopping và Trainer\n",
    "\n",
    "Tham số:\n",
    "không\n",
    "\n",
    "Trả về: \n",
    "logger: TensorBoardLogger\n",
    "checkpoint: theo dõi val_loss, lưu toàn bộ giá trị model tốt nhất\n",
    "earlystop: theo dõi val_f1, dừng nếu 3 epochs không tăng 0.005\n",
    "trainer: chạy trên auto accelerator, tối đa 10 epoch\n",
    "'''\n",
    "def log_check_early():\n",
    "    logger = TensorBoardLogger(save_dir='tb_log')\n",
    "    checkpoint = ModelCheckpoint(monitor='val_loss', mode = 'min', save_top_k=1, save_weights_only=False)\n",
    "    earlystop = EarlyStopping(monitor='val_f1', mode = 'max', min_delta = 0.005, patience=3)\n",
    "    trainer = PL.Trainer(accelerator='auto',\n",
    "                        logger = logger,\n",
    "                        callbacks = [checkpoint, earlystop],\n",
    "                        max_epochs=10)\n",
    "    return logger, checkpoint, earlystop, trainer\n",
    "# tuner = Tuner(trainer)\n",
    "# lr_finder = tuner.lr_find(model, datamodule=datamodule, min_lr = 0.001, max_lr = 0.01)\n",
    "# model.hparams.lr = lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90388986",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = datamodule()\n",
    "model = TinyVGG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "43bb0a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup()\n",
    "train_loader = datamodule.train_dataloader()\n",
    "test_loader = datamodule.test_dataloader()\n",
    "val_loader = datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d1eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "443716dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3, 2)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4a60a5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyVGG                                  [1, 3]                    --\n",
       "├─Sequential: 1-1                        [1, 10, 32, 32]           --\n",
       "│    └─Conv2d: 2-1                       [1, 10, 64, 64]           280\n",
       "│    └─ReLU: 2-2                         [1, 10, 64, 64]           --\n",
       "│    └─Conv2d: 2-3                       [1, 10, 64, 64]           910\n",
       "│    └─ReLU: 2-4                         [1, 10, 64, 64]           --\n",
       "│    └─MaxPool2d: 2-5                    [1, 10, 32, 32]           --\n",
       "├─Sequential: 1-2                        [1, 10, 16, 16]           --\n",
       "│    └─Conv2d: 2-6                       [1, 10, 32, 32]           910\n",
       "│    └─ReLU: 2-7                         [1, 10, 32, 32]           --\n",
       "│    └─Conv2d: 2-8                       [1, 10, 32, 32]           910\n",
       "│    └─ReLU: 2-9                         [1, 10, 32, 32]           --\n",
       "│    └─MaxPool2d: 2-10                   [1, 10, 16, 16]           --\n",
       "├─Sequential: 1-3                        [1, 3]                    --\n",
       "│    └─Flatten: 2-11                     [1, 2560]                 --\n",
       "│    └─Linear: 2-12                      [1, 3]                    7,683\n",
       "==========================================================================================\n",
       "Total params: 10,693\n",
       "Trainable params: 10,693\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 6.75\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.82\n",
       "Params size (MB): 0.04\n",
       "Estimated Total Size (MB): 0.91\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, [1,3,64,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e687f701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | train_f1       | MulticlassF1Score | 0      | train\n",
      "1 | val_f1         | MulticlassF1Score | 0      | train\n",
      "2 | test_f1        | MulticlassF1Score | 0      | train\n",
      "3 | block_1        | Sequential        | 1.2 K  | train\n",
      "4 | block_2        | Sequential        | 1.8 K  | train\n",
      "5 | classification | Sequential        | 7.7 K  | train\n",
      "-------------------------------------------------------------\n",
      "10.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.7 K    Total params\n",
      "0.043     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\envs\\DL_ENV\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:428: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\envs\\DL_ENV\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:428: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "d:\\Python\\envs\\DL_ENV\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 6/6 [00:05<00:00,  1.05it/s, v_num=0, train_loss=0.683, val_loss=0.886, val_f1=0.667]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "afd0858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = next(iter(train_loader))\n",
    "img = X[0].permute(1,2,0)\n",
    "img = img.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f15179b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
